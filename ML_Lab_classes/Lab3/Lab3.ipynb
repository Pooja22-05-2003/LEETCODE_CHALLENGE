{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given\n",
    "# target values/true labels\n",
    "target_values=[1,0,0,1,0,0,1,0,0,1,1,0,0,0,0,1,0,1,0,1]\n",
    "\n",
    "# assumed predictions done by machine learning model\n",
    "prediction_probabilities=[0.886,0.375,0.174,0.817,0.574,0.319,0.812,0.314,0.098,0.741,0.847,0.202,0.31,0.073,0.179,0.917,0.64,0.388,0.116,0.72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict() function accept 2 inputs, i.e. a list of prediction probabilities, and threshold value (as a float 0.5)\n",
    "# compute the final predictions to be output by the model and return them as a list\n",
    "# such that:    If a prediction probability <= threshold value, then the prediction is 0, and\n",
    "#               If a prediction probability > threshold value, then the prediction is 1\n",
    "\n",
    "def predict(prediction_prababilities, threshold_value):\n",
    "    pred=[]\n",
    "    for i in prediction_prababilities:\n",
    "        if(i>threshold_value):\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# evaluating model predictions using predict() function\n",
    "\n",
    "threshold_val=0.5\n",
    "model_predictions=predict(prediction_probabilities,threshold_val)\n",
    "print(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_score() function accept 2 inputs, i.e. a list of true labels and a list of model predictions\n",
    "# it calculate the model accuracy score using the true labels as compared to the predictions\n",
    "\n",
    "def acc_score(true_labels, model_predictions):\n",
    "    correct_count=0\n",
    "    total_len=len(true_labels)\n",
    "\n",
    "    for i in range(total_len):\n",
    "        if(true_labels[i]==model_predictions[i]):\n",
    "            correct_count+=1\n",
    "    \n",
    "    accuracy=correct_count/total_len\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating accuracy score using acc_score() function\n",
    "\n",
    "accuracy=acc_score(target_values, model_predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating accuracy score using accuracy_score() function from sklearn library\n",
    "\n",
    "sklearn_accuracy = accuracy_score(target_values, model_predictions)\n",
    "sklearn_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
